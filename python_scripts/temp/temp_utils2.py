import re
from typing import List
from fast_bunkai import FastBunkai


def split_sentences(text):
    sentences = []
    last = 0
    # å¥ç‚¹ã§æ–‡ã‚’åˆ†å‰²
    for match in re.finditer(r'[ã€‚ï¼ï¼Ÿâ€¦]', text):
        end = match.end()
        # å¥ç‚¹ã®ç›´å¾Œã«ç¶šãæ”¹è¡Œã‚’å«ã‚ã‚‹
        while end < len(text) and text[end] == '\n':
            end += 1
        sentence = text[last:end]
        sentences.append(sentence)
        last = end
    # æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
    if last < len(text):
        remaining = text[last:]
        sentences.append(remaining)
    # å„æ–‡å†…ã®æ”¹è¡Œã‚’é©åˆ‡ã«åˆ†å‰²
    final_sentences = []
    for s in sentences:
      if '\n' in s:
          parts = s.split('\n')
          for i, part in enumerate(parts):
              if part:
                  # æœ€å¾Œã®éƒ¨åˆ†ã§ãªã‘ã‚Œã°æ”¹è¡Œã‚’è¿½åŠ 
                  if i < len(parts) - 1:
                      final_sentences.append(part + '\n')
                  else:
                      final_sentences.append(part)
              # æ”¹è¡Œè‡ªä½“ã‚’ä¿æŒ
              if i < len(parts) - 1:
                  final_sentences.append('\n')
      else:
          final_sentences.append(s)
    return final_sentences


def split_sentences_ja(text: str) -> List[str]:
    """
    Split Japanese text into sentences using FastBunkai.
    
    FastBunkai provides excellent speed and accuracy for punctuated or emoji-rich text.
    For casual/spoken-style text with spaces instead of periods (common in transcripts or chats),
    we apply a lightweight preprocessing step: replace single spaces surrounded by Japanese characters
    with a period (ã€‚) to guide the splitter toward natural clause boundaries.
    
    This keeps the implementation generic, reusable, and minimalâ€”no heavy dependencies beyond fast_bunkai.
    
    Args:
        text: The Japanese text to split.
    
    Returns:
        A list of sentences as strings (stripped of whitespace).
    
    Example:
        >>> text = "3äººã®å…ˆç”Ÿã‹ã‚‰é›»è©±ãŒã‚ã£ãŸ è¿‘åœ°ãªã‚“ã‹å¿ƒå½“ãŸã‚Šã‚ã‚‹?"
        >>> split_sentences_ja(text)
        ['3äººã®å…ˆç”Ÿã‹ã‚‰é›»è©±ãŒã‚ã£ãŸ', 'è¿‘åœ°ãªã‚“ã‹å¿ƒå½“ãŸã‚Šã‚ã‚‹?']
        
        >>> text = "ç¾½ç”°ã‹ã‚‰âœˆï¸å‡ºç™ºã—ã¦ã€å‹ã ã¡ã¨ğŸ£é£Ÿã¹ã¾ã—ãŸã€‚æœ€é«˜ï¼ã¾ãŸè¡ŒããŸã„ãªğŸ˜‚ã§ã‚‚ã€äºˆç®—ã¯å¤§ä¸ˆå¤«ã‹ãªâ€¦?"
        >>> split_sentences_ja(text)
        ['ç¾½ç”°ã‹ã‚‰âœˆï¸å‡ºç™ºã—ã¦ã€å‹ã ã¡ã¨ğŸ£é£Ÿã¹ã¾ã—ãŸã€‚', 'æœ€é«˜ï¼', 'ã¾ãŸè¡ŒããŸã„ãªğŸ˜‚', 'ã§ã‚‚ã€äºˆç®—ã¯å¤§ä¸ˆå¤«ã‹ãªâ€¦?']
    """
    import re
    
    # Preprocess: treat isolated spaces (common in informal text) as potential sentence breaks
    # Only replace spaces that are between Japanese chars (hiragana, katakana, kanji, some punctuation)
    text = re.sub(r'([\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\u3000-\u303F])[ ]+([\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\u3000-\u303F])',
                  r'\1ã€‚\2', text)
    
    splitter = FastBunkai()
    sentences = list(splitter(text))
    return [s.strip() for s in sentences if s.strip()]
